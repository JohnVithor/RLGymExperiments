{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "import copy \n",
    "from src.Sarsa import SarsaAgent\n",
    "from src.SarsaLambda import SarsaLambdaAgent\n",
    "from src.QLearning import QLearningAgent\n",
    "from src.QLearningLambda import QLearningLambdaAgent\n",
    "from src.Visualizing import training_visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay: float = start_epsilon / (n_episodes / 2)\n",
    "final_epsilon = 0.0\n",
    "lambda_factor = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: gym.Env = gym.make('CliffWalking-v0', max_episode_steps=1_000)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "\n",
    "agent_ql = QLearningAgent(\n",
    "    action_space=copy.deepcopy(env.action_space),\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "agent_sarsa = SarsaAgent(\n",
    "    action_space=copy.deepcopy(env.action_space),\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "agent_ql_lambda = QLearningLambdaAgent(\n",
    "    action_space=copy.deepcopy(env.action_space),\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    lambda_factor=lambda_factor,\n",
    ")\n",
    "agent_sarsa_lambda = SarsaLambdaAgent(\n",
    "    action_space=copy.deepcopy(env.action_space),\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    lambda_factor=lambda_factor,\n",
    ")\n",
    "\n",
    "actions: list[str] = [\"Up\", \"Right\", \"Down\", \"Left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(n_episodes)):\n",
    "    curr_observation, info = env.reset()\n",
    "    curr_action: int = agent_ql.get_action(curr_observation)\n",
    "    # play one episode\n",
    "    while True:\n",
    "        # act upon the enviromment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(curr_action)\n",
    "        is_terminal: bool = terminated or truncated\n",
    "        # select next action\n",
    "        next_action: int = agent_ql.get_action(next_observation)\n",
    "        # update the agent\n",
    "        agent_ql.update(curr_observation, curr_action, reward, terminated, next_observation, next_action)\n",
    "        # update the current observation and action\n",
    "        curr_observation = next_observation\n",
    "        curr_action = next_action\n",
    "        # end the episode\n",
    "        if (is_terminal):\n",
    "            break\n",
    "    # reduce exploration factor\n",
    "    agent_ql.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_visualize(env, agent_ql, 'green', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(n_episodes)):\n",
    "    curr_observation, info = env.reset()\n",
    "    curr_action: int = agent_sarsa.get_action(curr_observation)\n",
    "    # play one episode\n",
    "    while True:\n",
    "        # act upon the enviromment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(curr_action)\n",
    "        is_terminal: bool = terminated or truncated\n",
    "        # select next action\n",
    "        next_action: int = agent_sarsa.get_action(next_observation)\n",
    "        # update the agent\n",
    "        agent_sarsa.update(curr_observation, curr_action, reward, terminated, next_observation, next_action)\n",
    "        # update the current observation and action\n",
    "        curr_observation = next_observation\n",
    "        curr_action = next_action\n",
    "        # end the episode\n",
    "        if (is_terminal):\n",
    "            break\n",
    "    # reduce exploration factor\n",
    "    agent_sarsa.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_visualize(env, agent_sarsa, 'yellow',10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(n_episodes)):\n",
    "    curr_observation, info = env.reset()\n",
    "    curr_action: int = agent_ql_lambda.get_action(curr_observation)\n",
    "    # play one episode\n",
    "    while True:\n",
    "        # act upon the enviromment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(curr_action)\n",
    "        is_terminal: bool = terminated or truncated\n",
    "        # select next action\n",
    "        next_action: int = agent_ql_lambda.get_action(next_observation)\n",
    "        # update the agent\n",
    "        agent_ql_lambda.update(curr_observation, curr_action, reward, terminated, next_observation, next_action)\n",
    "        # update the current observation and action\n",
    "        curr_observation = next_observation\n",
    "        curr_action = next_action\n",
    "        # end the episode\n",
    "        if (is_terminal):\n",
    "            break\n",
    "    # reduce exploration factor\n",
    "    agent_ql_lambda.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_visualize(env, agent_ql_lambda, 'green', 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(n_episodes)):\n",
    "    curr_observation, info = env.reset()\n",
    "    curr_action: int = agent_sarsa_lambda.get_action(curr_observation)\n",
    "    # play one episode\n",
    "    while True:\n",
    "        # act upon the enviromment\n",
    "        next_observation, reward, terminated, truncated, info = env.step(curr_action)\n",
    "        is_terminal: bool = terminated or truncated\n",
    "        # select next action\n",
    "        next_action: int = agent_sarsa_lambda.get_action(next_observation)\n",
    "        # update the agent\n",
    "        agent_sarsa_lambda.update(curr_observation, curr_action, reward, terminated, next_observation, next_action)\n",
    "        # update the current observation and action\n",
    "        curr_observation = next_observation\n",
    "        curr_action = next_action\n",
    "        # end the episode\n",
    "        if (is_terminal):\n",
    "            break\n",
    "    # reduce exploration factor\n",
    "    agent_sarsa_lambda.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_visualize(env, agent_sarsa_lambda, 'yellow', 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agent_ql\n",
    "curr_obs, info = env.reset()\n",
    "# play one episode\n",
    "while True:\n",
    "    next_action: int = agent.get_action(curr_obs)\n",
    "    next_obs, reward, terminated, truncated, info = env.step(next_action)\n",
    "    curr_obs = next_obs\n",
    "    if (terminated or truncated):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
